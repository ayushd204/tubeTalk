{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushd204/tubeTalk/blob/side-branch/main_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl5M4SiVEr7j"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# CELL 1: INSTALLING DEPENDENCIES (UPDATED)\n",
        "# ---\n",
        "!pip install --upgrade -q langchain langchain-community langchain-core langchain_google_genai google-generativeai==0.1.3 faiss-cpu youtube_transcript_api sentence-transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX2M9L7Kh9Ix"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# CELL 2: IMPORTING LIBRARIES\n",
        "# ---\n",
        "# We've added 'getpass' for secure API key entry, re' for URL processing,\n",
        "# 'os' for file system checks, and more from 'gradio' and 'langchain'.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import gradio as gr\n",
        "\n",
        "# For transcript loading\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "# For LangChain components\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHzLicgiIh8v"
      },
      "outputs": [],
      "source": [
        "# CELL 3: SETTING UP THE GOOGLE API KEY\n",
        "# This uses getpass to create a secure password-style prompt\n",
        "# (Remember to generate a new key and delete the one you exposed)\n",
        "if 'GOOGLE_API_KEY' not in os.environ:\n",
        "    os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3S2drSWTOEh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeyFpTn9R4u8"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "\n",
        "# Make sure you have run CELL 3 to define GOOGLE_API_KEY\n",
        "try:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except NameError:\n",
        "    print(\"Error: GOOGLE_API_KEY is not defined. Please run Cell 3 first.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during configuration: {e}\")\n",
        "\n",
        "print(\"--- Listing Available Google Models ---\")\n",
        "\n",
        "try:\n",
        "    for m in genai.list_models():\n",
        "        # 'generateContent' is for text/chat (LLM)\n",
        "        if 'generateContent' in m.supported_generation_methods:\n",
        "            print(f\"\\nâœ… Text Model: {m.name}\")\n",
        "            print(f\"   Description: {textwrap.shorten(m.description, width=100)}\")\n",
        "\n",
        "        # 'embedContent' is for embeddings (like the one you replaced)\n",
        "        if 'embedContent' in m.supported_generation_methods:\n",
        "            print(f\"\\nâœ… Embedding Model: {m.name}\")\n",
        "            print(f\"   Description: {textwrap.shorten(m.description, width=100)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while listing models: {e}\")\n",
        "    print(\"Please ensure your API key is correct and has the 'Generative Language API' enabled in your Google Cloud project.\")\n",
        "\n",
        "print(\"\\n-----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozk_KambRqfj"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# CELL 4: DEFINING CORE HELPER FUNCTIONS\n",
        "# ---\n",
        "# These are the reusable building blocks of our application.\n",
        "\n",
        "# This is the same function you had for loading the embedding model.\n",
        "def download_embeddings():\n",
        "    \"\"\"Downloads the sentence transformer model.\"\"\"\n",
        "    print(\"Downloading embedding model...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    print(\"Embedding model downloaded.\")\n",
        "    return embeddings\n",
        "\n",
        "def get_video_id(url):\n",
        "    \"\"\"\n",
        "    Helper function to extract the video ID from various YouTube URL formats.\n",
        "    This is new and necessary so the user can paste any YouTube link.\n",
        "    \"\"\"\n",
        "    # Regex to find video ID in various URL formats\n",
        "    regex = (\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?\"\n",
        "        r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|\"\n",
        "        r\"youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    )\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def create_rag_chain(vector_store):\n",
        "    \"\"\"\n",
        "    Creates the full RAG (Retrieval-Augmented Generation) chain.\n",
        "    This replaces the logic you had inside your old 'chatbot_interface'.\n",
        "    It's better practice to define the chain separately.\n",
        "    \"\"\"\n",
        "    # 1. Define the Retriever\n",
        "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_k=3)\n",
        "\n",
        "    # 2. Define the Prompt Template\n",
        "    template = \"\"\"\n",
        "    You are a helpful YouTube assistant. Use the following context to answer the user's question.\n",
        "    If you don't know the answer, just say \"I don't know.\" Do not try to make up an answer.\n",
        "    Answer in a conversational and helpful tone.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    # 3. Define the LLM\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # 4. Create the Chain using LangChain Expression Language (LCEL)\n",
        "    # This is a more modern and robust way to build your chain.\n",
        "\n",
        "    # This part runs in parallel:\n",
        "    # - \"context\": The user's question is passed to the retriever to get relevant docs.\n",
        "    # - \"question\": The user's question is passed through unchanged.\n",
        "    setup_and_retrieval = RunnableParallel(\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    )\n",
        "\n",
        "    # The output of the parallel step is fed into the prompt,\n",
        "    # then to the LLM, and finally parsed as a string.\n",
        "    chain = setup_and_retrieval | prompt | llm | StrOutputParser()\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edUN8m7oVBKB"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# CELL 5: DEFINING GRADIO UI FUNCTIONS\n",
        "# ---\n",
        "# These are the functions that Gradio will call directly.\n",
        "\n",
        "# We'll use this constant to store our saved index\n",
        "INDEX_NAME = \"faiss_video_index\"\n",
        "\n",
        "def process_video(url, embeddings_model):\n",
        "    \"\"\"\n",
        "    This function is called when the user clicks \"Process Video\".\n",
        "\n",
        "    *** FIX ***: We are now returning 3 values to update each\n",
        "    component (status, chatbot, textbox) individually,\n",
        "    avoiding the buggy gr.Group.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        # Returns: (status_msg, chatbot_update, textbox_update)\n",
        "        return (\n",
        "            \"Please enter a YouTube URL.\",\n",
        "            gr.update(value=[]),\n",
        "            gr.update(interactive=False)\n",
        "        )\n",
        "\n",
        "    print(f\"Processing video: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get Video ID\n",
        "        video_id = get_video_id(url)\n",
        "        if not video_id:\n",
        "            return (\n",
        "                \"Error: Invalid YouTube URL.\",\n",
        "                gr.update(value=[]),\n",
        "                gr.update(interactive=False)\n",
        "            )\n",
        "\n",
        "        print(f\"Extracted Video ID: {video_id}\")\n",
        "\n",
        "        # Step 2: Load and Split Documents using YoutubeLoader\n",
        "        print(\"Loading transcript with YoutubeLoader...\")\n",
        "        loader = YoutubeLoader(video_id=video_id, add_video_info=False)\n",
        "        docs = loader.load()\n",
        "\n",
        "        if not docs:\n",
        "            print(\"Error: No transcript found or loader failed.\")\n",
        "            return (\n",
        "                \"Error: Could not load transcript from this video.\",\n",
        "                gr.update(value=[]),\n",
        "                gr.update(interactive=False)\n",
        "            )\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        chunks = text_splitter.split_documents(docs)\n",
        "        print(f\"Transcript split into {len(chunks)} chunks.\")\n",
        "\n",
        "        # Step 3: Create and Save Vector Store\n",
        "        if os.path.exists(INDEX_NAME):\n",
        "            print(f\"Removing old index: {INDEX_NAME}\")\n",
        "            import shutil\n",
        "            shutil.rmtree(INDEX_NAME)\n",
        "\n",
        "        print(\"Creating new vector store...\")\n",
        "        vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings_model)\n",
        "\n",
        "        vector_store.save_local(INDEX_NAME)\n",
        "        print(f\"Vector store saved to disk as '{INDEX_NAME}'.\")\n",
        "\n",
        "        # Return a success message and enable the chat textbox\n",
        "        success_message = f\"âœ… Successfully processed video! ({len(chunks)} chunks created). You can now ask questions.\"\n",
        "\n",
        "        # Returns: (status_msg, chatbot_update, textbox_update)\n",
        "        return (\n",
        "            success_message,\n",
        "            gr.update(value=[]), # Clears the chatbot window\n",
        "            gr.update(interactive=True, placeholder=\"Ask your question here...\") # Enables the textbox\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unknown error occurred: {e}\")\n",
        "        # Returns: (status_msg, chatbot_update, textbox_update)\n",
        "        return (\n",
        "            f\"An error occurred: {str(e)}\",\n",
        "            gr.update(value=[]),\n",
        "            gr.update(interactive=False, placeholder=\"Error. Please reload.\")\n",
        "        )\n",
        "\n",
        "def respond_to_chat(question, history, embeddings_model):\n",
        "    \"\"\"\n",
        "    This is our chat function, called when the user hits Enter.\n",
        "    (This function is correct, no changes needed)\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(INDEX_NAME):\n",
        "        history.append((question, \"Error: Please process a video first.\"))\n",
        "        return \"\", history\n",
        "\n",
        "    try:\n",
        "        print(\"Loading vector store from disk...\")\n",
        "        vector_store = FAISS.load_local(\n",
        "            INDEX_NAME,\n",
        "            embeddings_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        rag_chain = create_rag_chain(vector_store)\n",
        "\n",
        "        print(f\"Invoking chain with question: {question}\")\n",
        "        response = rag_chain.invoke(question)\n",
        "        print(f\"Got response: {response}\")\n",
        "\n",
        "        history.append((question, response))\n",
        "\n",
        "        return \"\", history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during chat: {e}\")\n",
        "        history.append((question, f\"An error occurred: {str(e)}\"))\n",
        "        return \"\", history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a6nD45HubLlq",
        "outputId": "65bde349-f0b6-47d0-d28f-4363e4646282"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing to launch Gradio app...\n",
            "Downloading embedding model...\n",
            "Embedding model downloaded.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3651546689.py:39: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradio app is ready. Launching...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ab67f40f6f95d1b01e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ab67f40f6f95d1b01e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing video: https://www.youtube.com/watch?v=d0XKtUXgpOw\n",
            "Extracted Video ID: d0XKtUXgpOw\n",
            "Loading transcript with YoutubeLoader...\n",
            "Transcript split into 16 chunks.\n",
            "Removing old index: faiss_video_index\n",
            "Creating new vector store...\n",
            "Vector store saved to disk as 'faiss_video_index'.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1886: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading vector store from disk...\n",
            "Invoking chain with question: Are stocks discussed in the video?\n",
            "Got response: Yes, stocks are discussed in the video! The speaker mentions the value of all the stocks in the United States and even talks about owning a single stock.\n",
            "Loading vector store from disk...\n",
            "Invoking chain with question: are there dinosauruss in the video\n",
            "Got response: I don't know.\n",
            "Processing video: https://www.youtube.com/watch?v=d0XKtUXgpOw\n",
            "Extracted Video ID: d0XKtUXgpOw\n",
            "Loading transcript with YoutubeLoader...\n",
            "Transcript split into 16 chunks.\n",
            "Removing old index: faiss_video_index\n",
            "Creating new vector store...\n",
            "Vector store saved to disk as 'faiss_video_index'.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1886: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading vector store from disk...\n",
            "Invoking chain with question: Explain the video. Tell me the three main points of the video\n",
            "Got response: This video features Warren Buffett, who is the chairman, CEO, and largest shareholder of Berkshire Hathaway and considered one of the world's most successful investors. It seems to be part of a \"Mentor Me\" series, aiming to share insights from highly successful individuals.\n",
            "\n",
            "Here are three main points from the video:\n",
            "\n",
            "1.  **Cash is a bad investment:** Warren Buffett states that cash is always a bad investment because it doesn't produce anything and is sure to go down in value over time. He'd much rather have good businesses than surplus cash.\n",
            "2.  **Gold is not a productive asset:** He illustrates this by comparing the value of all the gold in the world ($7 trillion) to the value of productive assets like a third of all US stocks, all the Farmland in the US, seven Exxon Mobils, and a trillion dollars in cash. He emphasizes that gold just \"shines\" and doesn't *do* anything, unlike businesses or land that produce value.\n",
            "3.  **Invest in assets that produce value:** The real test of an investment, according to Buffett, is whether you'd be happy with it even if it never got quoted again, focusing on what the asset *does* for you. He suggests understanding businesses and their future potential to get rich.\n",
            "Loading vector store from disk...\n",
            "Invoking chain with question: MitÃ¤ videossa sanotaan dinosauruksista\n",
            "Got response: I don't know.\n"
          ]
        }
      ],
      "source": [
        "# ---\n",
        "# CELL 6: LAUNCHING THE GRADIO APPLICATION\n",
        "# ---\n",
        "# *** FIX ***: Removed the 'gr.Group' entirely.\n",
        "# We will now update the 'chatbot' and 'msg_textbox' components directly.\n",
        "\n",
        "print(\"Preparing to launch Gradio app...\")\n",
        "\n",
        "# Load the embedding model once when the app starts.\n",
        "embeddings = download_embeddings()\n",
        "\n",
        "# Define the Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ðŸŽ¬ TubeTalk: Ask Questions About Any YouTube Video\")\n",
        "    gr.Markdown(\n",
        "        \"**Instructions:**\\n\"\n",
        "        \"1. Paste a YouTube URL (with English transcripts) into the box below.\\n\"\n",
        "        \"2. Click the 'Process Video' button.\\n\"\n",
        "        \"3. Wait for the 'Successfully processed' message.\\n\"\n",
        "        \"4. Ask your questions in the chat window!\"\n",
        "    )\n",
        "\n",
        "    # Store the embeddings model in a 'State' variable\n",
        "    embedding_state = gr.State(embeddings)\n",
        "\n",
        "    # Section for Video Processing\n",
        "    with gr.Group():\n",
        "        with gr.Row():\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"YouTube URL\",\n",
        "                placeholder=\"https://www.youtube.com/watch?v=...\"\n",
        "            )\n",
        "            process_button = gr.Button(\"Process Video\", variant=\"primary\")\n",
        "\n",
        "        status_output = gr.Markdown() # For success/error messages\n",
        "\n",
        "    # Section for Chatting (Manual Build)\n",
        "    # *** FIX ***: No more gr.Group wrapper\n",
        "    chatbot = gr.Chatbot(\n",
        "        label=\"Video Q&A\",\n",
        "        height=400\n",
        "    )\n",
        "    msg_textbox = gr.Textbox(\n",
        "        label=\"Ask your question:\",\n",
        "        placeholder=\"Process a video first...\",\n",
        "        interactive=False # *** This is correct. Start as disabled ***\n",
        "    )\n",
        "\n",
        "    # --- Event Wiring ---\n",
        "\n",
        "    # 1. When the 'Process Video' button is clicked:\n",
        "    # *** FIX ***: Outputs now point to the 3 components\n",
        "    process_button.click(\n",
        "        fn=process_video,\n",
        "        inputs=[url_input, embedding_state],\n",
        "        outputs=[status_output, chatbot, msg_textbox]\n",
        "    )\n",
        "\n",
        "    # 2. When the user hits 'Enter' in the textbox:\n",
        "    # (This was already correct)\n",
        "    msg_textbox.submit(\n",
        "        fn=respond_to_chat,\n",
        "        inputs=[msg_textbox, chatbot, embedding_state],\n",
        "        outputs=[msg_textbox, chatbot]\n",
        "    )\n",
        "\n",
        "print(\"Gradio app is ready. Launching...\")\n",
        "# We no longer need the 'chat_group.interactive = False' line\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}