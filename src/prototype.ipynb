{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5761,
     "status": "ok",
     "timestamp": 1761596681396,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "sl5M4SiVEr7j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q youtube_transcript_api langchain-community faiss-cpu langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23444,
     "status": "ok",
     "timestamp": 1761596709251,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "oHzLicgiIh8v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi,TranscriptsDisabled\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkdb4HP3IUg2"
   },
   "source": [
    "**1. Document Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1761598321170,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "Ozk_KambRqfj"
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=eDCxDAYMZnw\n",
    "def doc_ingest(video_id):\n",
    "  try:\n",
    "    yt = YouTubeTranscriptApi()\n",
    "    transcript = yt.fetch(video_id,languages=['en'])\n",
    "    combined_text = \" \".join(chunk.text for chunk in transcript)\n",
    "    return combined_text\n",
    "    # combined all of the chunked transcripts into one string\n",
    "    # print(combined_text)\n",
    "  except TranscriptsDisabled:\n",
    "    print(\"No transcript is available for this video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJOySY68Z6qo"
   },
   "source": [
    "**2. Text Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1761598419405,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "edUN8m7oVBKB",
    "outputId": "86871212-9a4b-4f48-953a-bdefcc72e4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='When asked about his secret, he said something surprising. He claimed that he never focused on being better than his opponent, but only on perfecting his own technique every single day. This mindset'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def split_documents(docs,chunk_size=1000,chunk_overlap=200):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "  doc = Document(page_content=docs)#creating a document object cause that is what splitter accepts\n",
    "  text_chunks = text_splitter.split_documents([doc])\n",
    "  return text_chunks\n",
    "\n",
    "video_id = \"eDCxDAYMZnw\"\n",
    "combined_text = doc_ingest(video_id)\n",
    "# print(len(combined_text))\n",
    "\n",
    "split_chunks = split_documents(combined_text,200,50)\n",
    "print(split_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "395ybm2BaAqK"
   },
   "source": [
    "**3. Storing the chunks in a vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3569,
     "status": "ok",
     "timestamp": 1761597895025,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "8I9U2ukTaIOV",
    "outputId": "8606be16-1d5f-414e-852d-dc9c46015f24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/lpctk93j6dggsv6qs4cbgmx40000gn/T/ipykernel_62985/2552114570.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_embeddings()\n",
    "print(\"Embeddings model downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10577,
     "status": "ok",
     "timestamp": 1761598585360,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "a6nD45HubLlq",
    "outputId": "4a3dcfa7-c98b-4c18-a0d1-cf454a8d701c"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(split_chunks,embeddings)\n",
    "#convert the given chunks to respective vectors ; the vector ids are different every time!\n",
    "\n",
    "# print(vector_store.index_to_docstore_id)\n",
    "# chunks are respe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbZ4Cx0Ohsnh"
   },
   "source": [
    "**RETRIEVER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761599979958,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "LxUs2DRJhyt4",
    "outputId": "6a585362-85ce-4edf-d7f4-be27e7cff50c"
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "#using same vector store as a retriever which searches for semantic similarity and outputs 3 relevant blocks ;\n",
    "# retriever.invoke(\"what is blackhole ? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QELzKyttjRN7"
   },
   "source": [
    "**Setting up LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18862,
     "status": "error",
     "timestamp": 1761600508763,
     "user": {
      "displayName": "Ayush Dhungana",
      "userId": "02134317253512555312"
     },
     "user_tz": 300
    },
    "id": "Or3R5-icjQy4",
    "outputId": "769096d1-b9bb-429b-94d3-b1af45e29e9b"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Get API key from Colab user data secrets\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# print(result)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    Answer ONLY from the provided transcript context of the video.\n",
    "    If the context is insufficient, just say that you donot know the answer.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "#efficient prompt for llm questioning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and stances. They never stopped practicing the fundamentals even after they had learned advanced techniques that seemed more impressive and exciting. This goes against how most people approach\n",
      "\n",
      "the advanced techniques that look impressive to others. However, the samurai understood something crucial about how mastery actually works. Excellence in anything is not about knowing a thousand\n",
      "\n",
      "This goes against how most people approach learning today. We rush through the basics because they feel boring and simple. We want to move quickly to the advanced techniques that look impressive to\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Why were 90s programmers so legendary ?\")\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(content.page_content for content in retrieved_docs)\n",
    "    return context_text\n",
    "\n",
    "context = format_docs(retrieved_docs)\n",
    "print(context)\n",
    "question = \"Why were 90s programmers so legendary ?\"\n",
    "#so basically if the context retrieved is not flawless, the answer of the LLM wouldn't be good ; so it all depends upon the context provided by the retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not know the answer based on the provided transcript. The transcript discusses the learning approach of samurai and contrasts it with how people approach learning today, but it does not mention 90s programmers.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a chain!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a chain -- a pipeline so that none of the processes be manually invoked and the orchestration be handled by the pipeline itself ; \n",
    "\n",
    "from langchain_core.runnables import RunnableParallel,RunnableLambda,RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"context\":retriever | RunnableLambda(format_docs),\n",
    "    \"question\":RunnablePassthrough()\n",
    "}\n",
    ")\n",
    "# a parallel chain where :\n",
    "#retriever gets the question -- input -- and feeds it into format_docs (converted into runnable through runnablelambda)\n",
    "# parallel_chain.invoke(\"What is blackhole ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The path to mastery rests on three powerful foundations. The first foundation is to never stop perfecting the fundamentals, no matter how advanced you become. The second foundation begins with \"follow\" but is incomplete in the provided text. The third foundation is not detailed.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "main_chain = parallel_chain | prompt_template | llm | parser\n",
    "#parallel chain combined with prompt and llm to parse the ouptut ultimately forming a linear chain !\n",
    "main_chain.invoke('what does the path to mastery look like ?')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOHMzDJsvOWiZ59TeVlejda",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
